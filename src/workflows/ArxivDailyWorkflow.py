import json
import logging
import time
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Literal

from tqdm.asyncio import tqdm_asyncio

from src.ai.ArxivAnalyzer import ArxivAnalyzer
from src.ai.ArxivJudger import ArxivJudger
from src.config.Config import Config
from src.crawl.ArxivDailyCrawlService import ArxivDailyCrawlService
from src.models.Arxiv import ArxivPageResult, ArxivArticle
from src.models.Encoder import CustomEncoder
from src.utils.TimeUtils import TimeUtils


# ------------------------------------------------------------------
# æ—¥å¿—é…ç½®
# ------------------------------------------------------------------
def _setup_logger(category: str) -> logging.Logger:
    """
    ä¸ºæŒ‡å®š category åˆ›å»ºä¸€ä¸ªå¸¦æ»šåŠ¨æ–‡ä»¶çš„ logger
    """
    today_str = TimeUtils.current_date_str()
    log_dir = Path(Config.ANALYZE_REPORT_PATH) / today_str / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)

    logger = logging.getLogger(f"arxiv_daily.{category}")
    logger.setLevel(logging.DEBUG)

    # é¿å…é‡å¤ handler
    if logger.handlers:
        return logger

    fmt = "[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s"
    date_fmt = "%Y-%m-%d %H:%M:%S"

    # æ§åˆ¶å°
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    console.setFormatter(logging.Formatter(fmt, date_fmt))
    logger.addHandler(console)

    # æ–‡ä»¶ï¼ŒæŒ‰å¤©æ»šåŠ¨ï¼Œæœ€å¤§ 10 MB
    file_handler = RotatingFileHandler(
        log_dir / f"{category}.log",
        maxBytes=10 * 1024 * 1024,
        backupCount=3,
        encoding="utf-8",
    )
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(fmt, date_fmt))
    logger.addHandler(file_handler)

    return logger


# ------------------------------------------------------------------
# è¿›åº¦æ¡å‹å¥½çš„æ—¥å¿— handlerï¼ˆé˜²æ­¢ tqdm æŠ–åŠ¨ï¼‰
# ------------------------------------------------------------------
class AsyncTqdmLoggingHandler(logging.StreamHandler):
    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm_asyncio.write(msg)
            self.flush()
        except Exception:
            self.handleError(record)


# ------------------------------------------------------------------
# ä¸»æµç¨‹
# ------------------------------------------------------------------
class ArxivDailyWorkflow:
    def __init__(
            self,
            category: Literal[
                "cs.AI",
                "cs.AR",
                "cs.CC",
                "cs.CE",
                "cs.CG",
                "cs.CL",
                "cs.CR",
                "cs.CV",
                "cs.CY",
                "cs.DB",
                "cs.DC",
                "cs.DL",
                "cs.DM",
                "cs.DS",
                "cs.ET",
                "cs.FL",
                "cs.GL",
                "cs.GR",
                "cs.GT",
                "cs.HC",
                "cs.IR",
                "cs.IT",
                "cs.LG",
                "cs.LO",
                "cs.MA",
                "cs.MM",
                "cs.MS",
                "cs.NA",
                "cs.NE",
                "cs.NI",
                "cs.OH",
                "cs.OS",
                "cs.PF",
                "cs.PL",
                "cs.RO",
                "cs.SC",
                "cs.SD",
                "cs.SE",
                "cs.SI",
                "cs.SY",
            ],
            batchsize: int = 5,
    ):
        self.category = category
        self.batchsize = batchsize

        self.crawlResult: ArxivPageResult = ArxivPageResult(category=self.category)
        self.crawlService = ArxivDailyCrawlService(self.category)
        self.judgeService = ArxivJudger()
        self.aiService = ArxivAnalyzer()

        today = TimeUtils.current_date_str()
        self.folder = Path(Config.ANALYZE_REPORT_PATH) / today / self.category
        self.folder.mkdir(parents=True, exist_ok=True)

        self.logger = _setup_logger(category)

    # ------------------------------------------------------------------
    # 01 çˆ¬å–
    # ------------------------------------------------------------------
    async def crawl(self) -> ArxivPageResult:
        self.logger.info("ğŸ“¥ å¼€å§‹çˆ¬å– %s ä»Šæ—¥æ–‡ç« ...", self.category)
        start = time.perf_counter()

        try:
            self.crawlResult = await self.crawlService.crawl()
            elapsed = time.perf_counter() - start
            self.logger.info(
                "âœ… çˆ¬å–å®Œæˆï¼Œå…± %d ç¯‡æ–‡ç« ï¼Œè€—æ—¶ %.2f ç§’",
                len(self.crawlResult.articles),
                elapsed,
            )
            return self.crawlResult
        except Exception as e:
            self.logger.exception("âŒ çˆ¬å–å¤±è´¥ï¼š%s", e)
            raise

    # ------------------------------------------------------------------
    # 02 ç²—ç­›
    # ------------------------------------------------------------------
    async def _judge_one_article(self, article: ArxivArticle):
        try:
            return await self.judgeService.judge(article)
        except Exception as e:
            self.logger.error("âš ï¸ åˆ¤æ–­æ–‡ç«  %s å¤±è´¥ï¼š%s", article.id, e)
            return None

    async def judge_articles(self):
        articles = self.crawlResult.articles
        if not articles:
            self.logger.warning("âš ï¸ æ— æ–‡ç« å¯ç­›é€‰ï¼Œè·³è¿‡ judge é˜¶æ®µ")
            return

        self.logger.info("ğŸ” å¼€å§‹ç­›é€‰æ–‡ç« ï¼Œå…± %d ç¯‡...", len(articles))

        start = time.perf_counter()
        tasks = [
            self._judge_one_article(a) for a in articles
        ]
        results = await tqdm_asyncio.gather(*tasks, desc="Judging")

        succeed = 0
        for article, res in zip(articles, results):
            if res is None:
                continue
            article.judgerResult = res
            succeed += 1

        elapsed = time.perf_counter() - start
        self.logger.info(
            "âœ… ç­›é€‰å®Œæˆï¼š%d/%d æˆåŠŸï¼Œè€—æ—¶ %.2f ç§’",
            succeed,
            len(articles),
            elapsed,
        )

    # ------------------------------------------------------------------
    # 03 æ‹‰å–å…ƒæ•°æ®ï¼ˆä»… worth_readï¼‰
    # ------------------------------------------------------------------
    async def _generate_metadata(self, article: ArxivArticle):
        try:
            pdf_url = str(article.pdf_url)
            if not pdf_url:
                return None
            src_url = pdf_url.replace("pdf", "src")
            paths = await self.crawlService.download_attachment_async(src_url)
            files = await self.crawlService.extract_tar_gz(paths)
            metadata = await self.crawlService.process_file_lists(files)
            if len(metadata.figures) > Config.MAX_FIGURE_NUM:
                metadata.figures = metadata.figures[: Config.MAX_FIGURE_NUM]
            return metadata
        except Exception as e:
            self.logger.warning("âš ï¸ è·å– %s å…ƒæ•°æ®å¤±è´¥ï¼š%s", article.id, e)
            return None

    async def fill_meta_data(self):
        articles = [a for a in self.crawlResult.articles if a.judgerResult and a.judgerResult.worth_read]
        if not articles:
            self.logger.warning("âš ï¸ æ²¡æœ‰å€¼å¾—é˜…è¯»çš„æ–‡ç« ï¼Œè·³è¿‡å…ƒæ•°æ®æ‹‰å–")
            return

        self.logger.info("ğŸ—‚  å¼€å§‹æ‹‰å–å…ƒæ•°æ®ï¼Œå…± %d ç¯‡...", len(articles))

        start = time.perf_counter()
        tasks = [self._generate_metadata(a) for a in articles]
        results = await tqdm_asyncio.gather(*tasks, desc="Meta")

        succeed = 0
        for article, meta in zip(articles, results):
            if meta:
                article.metadata = meta
                succeed += 1

        elapsed = time.perf_counter() - start
        self.logger.info(
            "âœ… å…ƒæ•°æ®æ‹‰å–å®Œæˆï¼š%d/%d æˆåŠŸï¼Œè€—æ—¶ %.2f ç§’",
            succeed,
            len(articles),
            elapsed,
        )

    # ------------------------------------------------------------------
    # 04 AI æ·±åº¦åˆ†æ
    # ------------------------------------------------------------------
    async def _ai_analyze_one(self, article: ArxivArticle):
        try:
            return await self.aiService.analyze(article.metadata)
        except Exception as e:
            self.logger.error("âš ï¸ AI åˆ†æ %s å¤±è´¥ï¼š%s", article.id, e)
            return None

    async def _write_and_analyze_one(self, article: ArxivArticle):
        try:
            analyzeResult = await self._ai_analyze_one(article)
            if not analyzeResult:
                return None
            filename = f"{article.title}.md"
            safe_filename = "".join(c for c in filename if c.isalnum() or c in " ._-")
            out_path = self.folder / safe_filename
            with open(out_path, "w", encoding="utf-8") as f:
                f.write(f"# {article.title}\n")
                f.write(analyzeResult.text)
            return analyzeResult
        except Exception as e:
            self.logger.exception("âŒ ä¿å­˜åˆ†ææŠ¥å‘Š %s å¤±è´¥ï¼š%s", article.id, e)
            return None

    async def analyze(self):
        articles = [
            a for a in self.crawlResult.articles
            if a.judgerResult and a.judgerResult.worth_read and a.metadata
        ]
        if not articles:
            self.logger.warning("âš ï¸ æ— å¯åˆ†æçš„æ–‡ç« ï¼Œè·³è¿‡ AI åˆ†æé˜¶æ®µ")
            return

        self.logger.info("ğŸ¤– å¼€å§‹ AI åˆ†æï¼Œå…± %d ç¯‡...", len(articles))

        start = time.perf_counter()
        tasks = [self._write_and_analyze_one(a) for a in articles]
        await tqdm_asyncio.gather(*tasks, desc="Analyze")

        elapsed = time.perf_counter() - start
        self.logger.info("âœ… AI åˆ†æå®Œæˆï¼Œè€—æ—¶ %.2f ç§’", elapsed)

    # ------------------------------------------------------------------
    # 05 å¯¼å‡º JSON
    # ------------------------------------------------------------------
    async def save_json(self):
        outfile = self.folder / f"{self.category}.json"
        self.logger.info("ğŸ’¾ å¯¼å‡º JSON åˆ° %s ...", outfile)
        try:
            with open(outfile, "w", encoding="utf-8") as f:
                json.dump(
                    self.crawlResult.model_dump(exclude={
                        "articles": {
                            "__all__": {"metadata"}
                        }
                    }),
                    f,
                    ensure_ascii=False,
                    cls=CustomEncoder,
                    indent=2,
                )
            self.logger.info("âœ… JSON å¯¼å‡ºå®Œæˆ")
        except Exception as e:
            self.logger.exception("âŒ JSON å¯¼å‡ºå¤±è´¥ï¼š%s", e)

    async def run(self, without_analyze: bool = False):
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´å·¥ä½œæµï¼Œcategory=%s", self.category)
        try:
            await self.crawl()
            await self.judge_articles()
            if not without_analyze:
                await self.fill_meta_data()
                await self.analyze()
            await self.save_json()
            self.logger.info("ğŸ‰ å…¨éƒ¨æµç¨‹å®Œæˆï¼")
        except Exception as e:
            self.logger.exception("ğŸ’¥ å·¥ä½œæµå¼‚å¸¸ç»ˆæ­¢ï¼š%s", e)
            raise
